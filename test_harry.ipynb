{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, MaxPool2D, BatchNormalization, Flatten, GlobalAveragePooling2D, Input)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.applications import EfficientNetB7, MobileNetV2, VGG19, DenseNet121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory(path):\n",
    "    df = []\n",
    "    lowercase_chars = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    uppercase_chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Error: Directory not found: {path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    dir_list = os.listdir(path)\n",
    "    \n",
    "    for child in dir_list:\n",
    "        child_path = os.path.join(path, child)\n",
    "        child_name = child.split('_')[0]\n",
    "        \n",
    "        if not child_name in lowercase_chars:\n",
    "            continue\n",
    "        \n",
    "        for img_path in os.listdir(child_path):\n",
    "            direct = os.path.join(child_path, img_path)\n",
    "            df.append([direct, child_name])\n",
    "        \n",
    "    df = pd.DataFrame(df, columns=['image', 'label'])\n",
    "    print(\"The number of samples found: \", len(df))\n",
    "    \n",
    "    return df.copy()\n",
    "\n",
    "# Driver code\n",
    "dataset_path = 'data/raw/character_set3/training_data/'\n",
    "\n",
    "df = load_directory(dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_char = df['label'].value_counts().sort_index()\n",
    "print(sorted_by_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df['image'], df['label']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.30, random_state=41)\n",
    "training_df = pd.concat((X_train, Y_train), axis=1)\n",
    "testing_df = pd.concat((X_test, Y_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = training_df['image'], training_df['label']\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(x, y, test_size=0.25, random_state=41)\n",
    "training_df = pd.concat((X_train, Y_train), axis=1)\n",
    "validation_df = pd.concat((X_valid, Y_valid), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (32,32)\n",
    "IMG_SIZE = (32,32,3)\n",
    "BATCH_SIZE = 32\n",
    "opt = Adam(learning_rate=0.00001, epsilon=1e-6)\n",
    "loss = 'categorical_crossentropy'\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(dtype=np.int32, brightness_range=[0.0,1.0], fill_mode='nearest')\n",
    "gen2 = ImageDataGenerator(dtype=np.int32, fill_mode='nearest')\n",
    "train_gen = gen.flow_from_dataframe(training_df, x_col='image',y_col='label', batch_size=BATCH_SIZE, target_size=IMG_SHAPE)\n",
    "valid_gen = gen2.flow_from_dataframe(validation_df, x_col='image', y_col='label', batch_size=BATCH_SIZE,  target_size=IMG_SHAPE, shuffle=False)\n",
    "test_gen = gen2.flow_from_dataframe(testing_df, x_col='image', y_col='label', batch_size=BATCH_SIZE, target_size=IMG_SHAPE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = train_gen.class_indices\n",
    "mapping_inverse = dict(map(lambda x: tuple(reversed(x)), mapping.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, label=None) -> None:\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_NUM = 10\n",
    "IMG_NUM = 2\n",
    "show_image(train_gen[BATCH_NUM][0][IMG_NUM], mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM].argmax()])\n",
    "print('The shape of the image:', train_gen[BATCH_NUM][0][IMG_NUM].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=IMG_SIZE, batch_size=BATCH_SIZE, name='Input'),\n",
    "    Conv2D(3, (3,3), strides=1, activation='relu', padding='same'),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPool2D((3,3)),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(256, (3,3), strides=2, activation='relu', padding='same'),\n",
    "    MaxPool2D((2,2)),\n",
    "    Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(1024, (2,2), activation='relu', padding='same'),\n",
    "    MaxPool2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='selu'),\n",
    "    Dense(len(mapping), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clbck(model_name):\n",
    "    ERLY = EarlyStopping(patience=10, min_delta=0.01, start_from_epoch=10, verbose=1)\n",
    "    RD = ReduceLROnPlateau(patience=5, min_delta=0.01, factor=0.5)\n",
    "    CHK = ModelCheckpoint(f'{model_name}_model.keras',verbose=1, save_best_only=True)\n",
    "    return [ERLY,RD,CHK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_gen, epochs=EPOCHS, validation_data=valid_gen, callbacks=clbck(\"CustomCnn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title(\"Custom CNN Training VS. Validation performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_gen)\n",
    "pred = list(map(lambda x: mapping_inverse[np.argmax(x)], prediction))\n",
    "y_test = list(map(lambda x: mapping_inverse[x], test_gen.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\tThe Custom CNN Evaluation Performance')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_path = 'CustomCnn_model.keras'\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_regions(image, regions):\n",
    "    debug_image = image.copy()\n",
    "    for (x, y, w, h) in regions:\n",
    "        cv2.rectangle(debug_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Detected Text Regions\", debug_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_2_gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return gray_image\n",
    "\n",
    "def binarization(image):\n",
    "    img, thresh = cv2.threshold(image, 0,255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)\n",
    "    return img, thresh\n",
    "\n",
    "def dilate(image, words= False):\n",
    "    img = image.copy()\n",
    "    m = 3\n",
    "    n = m - 2                   # n less than m for Vertical structuring element to dilate chars\n",
    "    itrs = 4\n",
    "    if words:\n",
    "        m = 6\n",
    "        n = m\n",
    "        itrs = 3\n",
    "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (n, m))\n",
    "    dilation = cv2.dilate(img, rect_kernel, iterations = itrs)\n",
    "    return dilation\n",
    "\n",
    "def find_rect(image):\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    rects = []\n",
    "    \n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)  # Extract the bounding rectangle coordinates of each contour\n",
    "        rects.append([x,y,w,h])\n",
    "        \n",
    "    sorted_rects = list(sorted(rects, key=lambda x: x[0])) # Sorting the rects from Left-to-Right\n",
    "        \n",
    "    visualize_regions(image, rects)\n",
    "    \n",
    "    return sorted_rects\n",
    "\n",
    "def extract(image):\n",
    "    model = load_model()\n",
    "    chars = []              # a list to store recognized characters\n",
    "    \n",
    "    image_cpy = image.copy()\n",
    "    _, bin_img = binarization(convert_2_gray(image_cpy))\n",
    "    full_dil_img = dilate(bin_img,words=True)\n",
    "    words = find_rect(full_dil_img)                       # Recognized words within the image \n",
    "    del _, bin_img, full_dil_img                          # for better memory usage\n",
    "    \n",
    "    for word in words:\n",
    "        x,y,w,h = word                                    # coordinates of the word\n",
    "        img = image_cpy[y:y+h, x:x+w]\n",
    "        \n",
    "        _, bin_img = binarization(convert_2_gray(img))\n",
    "        dil_img = dilate(bin_img)\n",
    "        char_parts = find_rect(dil_img)                     # Recognized chars withtin the word\n",
    "        cv2.rectangle(image, (x,y),(x+w,y+h), (0,255,0), 3) # draw a green rectangle around the word\n",
    "        \n",
    "        del _, bin_img, dil_img\n",
    "        \n",
    "        for char in char_parts:    \n",
    "            x,y,w,h = char\n",
    "            ch = img[y:y+h, x:x+w]\n",
    "            \n",
    "            empty_img = np.full((32,32,1),255, dtype=np.uint8) # a white image used for resize with filling\n",
    "            x,y = 3,3                                          # starting indices\n",
    "            resized = cv2.resize(ch, (16,22), interpolation=cv2.INTER_CUBIC)\n",
    "            gray = convert_2_gray(resized)\n",
    "            empty_img[y:y+22, x:x+16,0] = gray.copy()          # integrate the recognized char into the white image\n",
    "            gray = cv2.cvtColor(empty_img, cv2.COLOR_GRAY2RGB)\n",
    "            gray = gray.astype(np.int32)\n",
    "            \n",
    "            cv2.imshow('Processed Character Image', empty_img)\n",
    "            cv2.waitKey(0)  # Wait indefinitely for a key press\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            predicted = mapping_inverse[np.argmax(model.predict(np.array([gray]), verbose=-1))]\n",
    "            chars.append(predicted)                            # append the character into the list\n",
    "            \n",
    "            del ch, resized, gray, empty_img\n",
    "        chars.append(' ')  # at the end of each iteration (end of word) append a space\n",
    "        \n",
    "    del model\n",
    "    show_image(image)\n",
    "    return ''.join(chars[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_bounding_boxes(regions):\n",
    "    # Sort by `y` first (top-to-bottom) with a threshold to group by rows\n",
    "    row_threshold = 20  # Adjust based on character spacing\n",
    "    regions = sorted(regions, key=lambda box: box[1])\n",
    "\n",
    "    # Group bounding boxes into rows\n",
    "    rows = []\n",
    "    current_row = [regions[0]]\n",
    "    \n",
    "    for i in range(1, len(regions)):\n",
    "        if abs(regions[i][1] - current_row[-1][1]) < row_threshold:\n",
    "            current_row.append(regions[i])\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [regions[i]]\n",
    "    rows.append(current_row)\n",
    "    \n",
    "    # Sort each row left-to-right\n",
    "    sorted_regions = []\n",
    "    for row in rows:\n",
    "        sorted_row = sorted(row, key=lambda box: box[0])\n",
    "        sorted_regions.extend(sorted_row)\n",
    "    \n",
    "    return sorted_regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text_regions(image):\n",
    "    # Apply image processing operation (e.g., thresholding, erosion, or dilation)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, processed_image = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # Find contours (regions of characters)\n",
    "    contours, _ = cv2.findContours(processed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    regions = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        if 0 < w < 100 and 0 < h < 100:  # Size filter\n",
    "            regions.append((x, y, w, h))\n",
    "    \n",
    "    # Visualize the image regions\n",
    "    visualize_regions(image, regions)\n",
    "    \n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_characters(image):\n",
    "    model = load_model()\n",
    "    regions = detect_text_regions(image)\n",
    "    \n",
    "    sorted_regions = sort_bounding_boxes(regions)\n",
    "    # print(sorted_regions)\n",
    "\n",
    "    characters = []\n",
    "    for (x, y, w, h) in sorted_regions:\n",
    "        char_image = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Check if the char_image is empty\n",
    "        if char_image.size == 0:\n",
    "            print(f\"Skipping empty character region at ({x}, {y}, {w}, {h})\")\n",
    "            continue  # Skip this iteration if the character image is empty\n",
    "        \n",
    "        # Print the shape of the character image for debugging\n",
    "        print(f\"Character image shape: {char_image.shape}, Character image region: ({x}, {y}, {w}, {h})\")\n",
    "        \n",
    "        empty_img = np.full((32, 32, 1), 255, dtype=np.uint8)  # a white image used for resize with filling\n",
    "        start_x, start_y = 3, 3  # starting indices\n",
    "        resized = cv2.resize(char_image, (16,22), interpolation=cv2.INTER_CUBIC)\n",
    "        gray = convert_2_gray(resized)\n",
    "        empty_img[start_y:start_y + 22, start_x:start_x + 16, 0] = gray.copy()  # integrate the recognized char into the white image\n",
    "        \n",
    "        # Convert to RGB format and adjust data type for prediction\n",
    "        gray = cv2.cvtColor(empty_img, cv2.COLOR_GRAY2RGB)\n",
    "        gray = gray.astype(np.int32)\n",
    "        \n",
    "        # Show the processed image\n",
    "        cv2.imshow('Processed Character Image', empty_img)\n",
    "        cv2.waitKey(0)  # Wait indefinitely for a key press\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        predicted = mapping_inverse[np.argmax(model.predict(np.array([gray]), verbose=-1))]\n",
    "        characters.append(predicted)\n",
    "\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('data/raw/character_set1/Test_5.png')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "y_pred = recognize_characters(image)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('data/raw/character_set1/Test_5.png')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "text = extract(image)\n",
    "print('-->',text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
